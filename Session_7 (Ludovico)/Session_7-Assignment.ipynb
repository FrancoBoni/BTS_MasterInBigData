{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import *\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/francobonifetto/Documents/BTS/Data Science Foundations/Session_5 (Ludovico)/Homework/One love.txt', 'r') as myfile:\n",
    "          onelove=myfile.read().replace('\\n', ' ')\n",
    "with open('/Users/francobonifetto/Documents/BTS/Data Science Foundations/Session_5 (Ludovico)/Homework/Be happy.txt', 'r') as myfile:\n",
    "          behappy=myfile.read().replace('\\n', ' ')\n",
    "with open('/Users/francobonifetto/Documents/BTS/Data Science Foundations/Session_5 (Ludovico)/Homework/Buffalo Soldier.txt', 'r') as myfile:\n",
    "          buffalosoldier=myfile.read().replace('\\n', ' ')\n",
    "with open('/Users/francobonifetto/Documents/BTS/Data Science Foundations/Session_5 (Ludovico)/Homework/Could you be loved.txt', 'r') as myfile:\n",
    "          couldyoubeloved=myfile.read().replace('\\n', ' ')\n",
    "with open('/Users/francobonifetto/Documents/BTS/Data Science Foundations/Session_5 (Ludovico)/Homework/Get up.txt', 'r') as myfile:\n",
    "          getup=myfile.read().replace('\\n', ' ')\n",
    "with open('/Users/francobonifetto/Documents/BTS/Data Science Foundations/Session_5 (Ludovico)/Homework/Is this love.txt', 'r') as myfile:\n",
    "          isthislove=myfile.read().replace('\\n', ' ')\n",
    "with open('/Users/francobonifetto/Documents/BTS/Data Science Foundations/Session_5 (Ludovico)/Homework/Jammin.txt', 'r') as myfile:\n",
    "          jammin=myfile.read().replace('\\n', ' ')\n",
    "with open('/Users/francobonifetto/Documents/BTS/Data Science Foundations/Session_5 (Ludovico)/Homework/Kaya.txt', 'r') as myfile:\n",
    "          kaya=myfile.read().replace('\\n', ' ')\n",
    "with open('/Users/francobonifetto/Documents/BTS/Data Science Foundations/Session_5 (Ludovico)/Homework/No woman no cry.txt', 'r') as myfile:\n",
    "          nowomannocry=myfile.read().replace('\\n', ' ')\n",
    "with open('/Users/francobonifetto/Documents/BTS/Data Science Foundations/Session_5 (Ludovico)/Homework/Stir it up.txt', 'r') as myfile:\n",
    "          stiritup=myfile.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_songs = [stiritup,nowomannocry, kaya, behappy, isthislove, getup, couldyoubeloved, buffalosoldier, behappy, onelove]\n",
    "\n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(all_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0.140644185274\n",
      "0 2 0.122648314915\n",
      "0 3 0.142867436721\n",
      "0 4 0.0695978357551\n",
      "0 5 0.209076927205\n",
      "0 6 0.0675134565523\n",
      "0 7 0.0726975575933\n",
      "0 8 0.142867436721\n",
      "0 9 0.062697200612\n",
      "1 2 0.06357271592\n",
      "1 3 0.150982151721\n",
      "1 4 0.0973409184515\n",
      "1 5 0.120189182529\n",
      "1 6 0.0913397684881\n",
      "1 7 0.0947532991239\n",
      "1 8 0.150982151721\n",
      "1 9 0.112176408645\n",
      "2 3 0.103368856411\n",
      "2 4 0.067511804204\n",
      "2 5 0.122193931697\n",
      "2 6 0.0505350074922\n",
      "2 7 0.0487715356341\n",
      "2 8 0.103368856411\n",
      "2 9 0.0812748882301\n",
      "3 4 0.102242749856\n",
      "3 5 0.142065420144\n",
      "3 6 0.114670790474\n",
      "3 7 0.0896599994987\n",
      "3 8 1.0\n",
      "3 9 0.1204870764\n",
      "4 5 0.0875792992077\n",
      "4 6 0.152316314338\n",
      "4 7 0.067531434548\n",
      "4 8 0.102242749856\n",
      "4 9 0.0845300292259\n",
      "5 6 0.0984215610838\n",
      "5 7 0.0694204477122\n",
      "5 8 0.142065420144\n",
      "5 9 0.155977239409\n",
      "6 7 0.0406814140217\n",
      "6 8 0.114670790474\n",
      "6 9 0.0559355084846\n",
      "7 8 0.0896599994987\n",
      "7 9 0.0785673682659\n",
      "8 9 0.1204870764\n"
     ]
    }
   ],
   "source": [
    "for s in range(10):\n",
    "    for t in range(s+1,10):\n",
    "        print(s,t, cosine_similarity(sklearn_representation.toarray()[s], sklearn_representation.toarray()[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most similar songs are number 0 and 5, which represent \"stir it up\" and \"get up\". The similarity is of 0.206123245582 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onelove_tokenized = word_tokenize(onelove)\n",
    "onelove_clean = [w for w in onelove_tokenized if w not in stopwords.words('english')]\n",
    "\n",
    "behappy_tokenized = word_tokenize(behappy)\n",
    "behappy_clean = [w for w in behappy_tokenized if w not in stopwords.words('english')]\n",
    "\n",
    "buffalosoldier_tokenized = word_tokenize(buffalosoldier)\n",
    "buffalosoldier_clean = [w for w in buffalosoldier_tokenized if w not in stopwords.words('english')]\n",
    "\n",
    "couldyoubeloved_tokenized = word_tokenize(couldyoubeloved)\n",
    "couldyoubeloved_clean = [w for w in couldyoubeloved_tokenized if w not in stopwords.words('english')]\n",
    "\n",
    "getup_tokenized = word_tokenize(getup)\n",
    "getup_clean = [w for w in getup_tokenized if w not in stopwords.words('english')]\n",
    "\n",
    "isthislove_tokenized = word_tokenize(isthislove)\n",
    "isthislove_clean = [w for w in isthislove_tokenized if w not in stopwords.words('english')]\n",
    "\n",
    "jammin_tokenized = word_tokenize(jammin)\n",
    "jammin_clean = [w for w in jammin_tokenized if w not in stopwords.words('english')]\n",
    "\n",
    "kaya_tokenized = word_tokenize(kaya)\n",
    "kaya_clean = [w for w in kaya_tokenized if w not in stopwords.words('english')]\n",
    "\n",
    "nowomannocry_tokenized = word_tokenize(nowomannocry)\n",
    "nowomannocry_clean = [w for w in nowomannocry_tokenized if w not in stopwords.words('english')]\n",
    "\n",
    "stiritup_tokenized = word_tokenize(stiritup)\n",
    "stiritup_clean = [w for w in stiritup_tokenized if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "stiritup_stemmed = [porter.stem(w) for w in stiritup_clean]\n",
    "nowomannocry_stemmed = [porter.stem(w) for w in nowomannocry_clean]\n",
    "kaya_stemmed = [porter.stem(w) for w in kaya_clean]\n",
    "jammin_stemmed = [porter.stem(w) for w in jammin_clean]\n",
    "isthislove_stemmed = [porter.stem(w) for w in isthislove_clean]\n",
    "getup_stemmed = [porter.stem(w) for w in getup_clean]\n",
    "couldyoubeloved_stemmed = [porter.stem(w) for w in couldyoubeloved_clean]\n",
    "buffalosoldier_stemmed = [porter.stem(w) for w in buffalosoldier_clean]\n",
    "behappy_stemmed = [porter.stem(w) for w in behappy_clean]\n",
    "onelove_stemmed = [porter.stem(w) for w in onelove_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_songs_stemmed = [onelove_stemmed,behappy_stemmed,buffalosoldier_stemmed,couldyoubeloved_stemmed,getup_stemmed,isthislove_stemmed,jammin_stemmed,kaya_stemmed,nowomannocry_stemmed,stiritup_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_songs = [' '.join(song) for song in all_songs_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sklearn_representation = sklearn_tfidf.fit_transform(join_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0.0932033311242\n",
      "0 2 0.0682791676848\n",
      "0 3 0.143581818445\n",
      "0 4 0.224212486213\n",
      "0 5 0.129944083348\n",
      "0 6 0.0841228181018\n",
      "0 7 0.0829819433043\n",
      "0 8 0.132373812232\n",
      "0 9 0.0770807453569\n",
      "1 2 0.0494083549615\n",
      "1 3 0.101829698121\n",
      "1 4 0.154040142903\n",
      "1 5 0.054225516412\n",
      "1 6 0.125792761432\n",
      "1 7 0.0693693431039\n",
      "1 8 0.101116492423\n",
      "1 9 0.0832599937482\n",
      "2 3 0.0428696332948\n",
      "2 4 0.0913281088424\n",
      "2 5 0.048645793631\n",
      "2 6 0.0710354350052\n",
      "2 7 0.0302987786785\n",
      "2 8 0.080587116268\n",
      "2 9 0.0748594343559\n",
      "3 4 0.192984151939\n",
      "3 5 0.247903863353\n",
      "3 6 0.118300800682\n",
      "3 7 0.0524121462287\n",
      "3 8 0.102183274992\n",
      "3 9 0.0782400472419\n",
      "4 5 0.168144279054\n",
      "4 6 0.178706464229\n",
      "4 7 0.092721166954\n",
      "4 8 0.180829914557\n",
      "4 9 0.0914607537492\n",
      "5 6 0.123340474609\n",
      "5 7 0.0735246196856\n",
      "5 8 0.110035690638\n",
      "5 9 0.107197514686\n",
      "6 7 0.062263121966\n",
      "6 8 0.138203997977\n",
      "6 9 0.119586433087\n",
      "7 8 0.0597933041652\n",
      "7 9 0.0615059738395\n",
      "8 9 0.157759703548\n"
     ]
    }
   ],
   "source": [
    "for s in range(10):\n",
    "    for t in range(s+1,10):\n",
    "        print(s,t, cosine_similarity(sklearn_representation.toarray()[s], sklearn_representation.toarray()[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that stopwords have been removed and stemmed, we can see that similarities have changed. \n",
    "#### We can see now what \"one love\" and \"could you be loved\" have over 0.2 in similarity, as well as \"buffalo soldier\" and \"get up\". \n",
    "#### I would have thought that Bob Marley's song have more in comun. We all know his songs for the word \"love\" and derivatives, but in #### the end there aren't to much alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "# initialize a new conditional distribution\n",
    "cfd = ConditionalFreqDist()\n",
    "\n",
    "# get a list of English stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag.lower() in ['nn','nns','nn$','nn-tl','nn+bez', 'nn+hvz', \n",
    "                           'nns$','np','np$','np+bez','nps', 'nps$','nr',\n",
    "                           'np-tl','nrs','nr$']\n",
    "\n",
    "for sentence in treebank.tagged_sents():\n",
    "    for (index, tagtuple) in enumerate(sentence):\n",
    "        (token, tag) = tagtuple\n",
    "        token = token.lower()\n",
    "        if token not in stopwords_list and is_noun(tag):\n",
    "            window = sentence[index+1:index+5]\n",
    "            for (window_token, window_tag) in window:\n",
    "                window_token = window_token.lower()\n",
    "                if window_token not in stopwords_list and is_noun(window_tag):\n",
    "                    cfd[token][window_token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chief\n"
     ]
    }
   ],
   "source": [
    "print(cfd['chairman'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%\n"
     ]
    }
   ],
   "source": [
    "print(cfd['profit'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chief\n"
     ]
    }
   ],
   "source": [
    "print(cfd['president'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonds\n"
     ]
    }
   ],
   "source": [
    "print(cfd['investors'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streets\n"
     ]
    }
   ],
   "source": [
    "print(cfd['city'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "officer\n"
     ]
    }
   ],
   "source": [
    "print(cfd['chief'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### it's quite normal to see chief as a word related to chariman and president, as both are related to the top hierarchy.\n",
    "#### Moreover, the frase \"chief executive oficer\" has grown in popularity so it's comun to see the following words after chief \"officer. Nevertheless, i think that it could have been any of those two: executive or officer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
